# ğŸ§ª RAGAS Evaluation Notebook

Este notebook muestra cÃ³mo evaluar un pipeline de **Retrieval-Augmented Generation (RAG)** utilizando **RAGAS**, una herramienta que permite cuantificar la calidad de las respuestas generadas y la efectividad del sistema de recuperaciÃ³n de contexto.

Se utilizan mÃ©tricas como:
- `context_recall` y `context_precision` para evaluar la **recuperaciÃ³n**.
- `faithfulness` y `answer_relevancy` para evaluar la **generaciÃ³n**.

## ğŸ“‚ Datasets utilizados

### ğŸ“š Base de conocimiento (chunked)
- **Fuente:** [`jamescalam/ai-arxiv2-chunks`](https://huggingface.co/datasets/jamescalam/ai-arxiv2-chunks)
- **DescripciÃ³n:** Dataset con abstracts y artÃ­culos cientÃ­ficos relacionados a IA, preprocesados y divididos en _chunks_ listos para ser embebidos en una base vectorial.

### âœ… Datos con ground truth
- **Fuente:** [`aurelio-ai/ai-arxiv2-ragas-mixtral`](https://huggingface.co/datasets/aurelio-ai/ai-arxiv2-ragas-mixtral)
- **DescripciÃ³n:** Dataset que incluye preguntas, respuestas correctas (_ground truth_) y documentos relevantes. Utilizado para evaluar el rendimiento de un pipeline RAG de forma supervisada.

## ğŸ› ï¸ Requisitos

- Python 3.10+
- API key de OpenAI
- Hugging Face Datasets
- LangChain
- RAGAS
- ChromaDB

## ğŸ” Variables de entorno

El archivo `.env` debe contener:

```env
OPENAI_API_KEY=tu_clave_de_openai
```

> âš ï¸ Este archivo estÃ¡ ignorado en `.gitignore` por seguridad.

## âš™ï¸ DescripciÃ³n del flujo

1. Se construye una base de conocimiento con embeddings OpenAI y ChromaDB.
2. Se configura un agente conversacional que recupera contexto desde la base vectorial.
3. Se responde una serie de preguntas reales.
4. Se evalÃºa automÃ¡ticamente el desempeÃ±o usando las mÃ©tricas de RAGAS.

---

ğŸ“Š Ideal para probar pipelines RAG en entornos controlados y medir su calidad con mÃ©tricas objetivas.