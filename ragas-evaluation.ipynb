{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQG4iSxVxw8f"
      },
      "source": [
        "# RAGAS Evaluation para LangChain Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eOnr6z_zLoc",
        "outputId": "b43767c1-4b53-498c-c21e-922b7d3c4696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.12.4\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov6TCS7bx1oI"
      },
      "source": [
        "**R**etrieval **A**ugmented **G**eneration **As**sessment (RAGAS) es un framework de evaluación diseñado para cuantificar el rendimiento de nuestros pipelines RAG (Retrieval-Augmented Generation). En este ejemplo, veremos cómo usarlo con un agente conversacional habilitado con RAG utilizando LangChain.\n",
        "\n",
        "Dado que necesitamos tanto un agente como un pipeline RAG para poder evaluar con RAGAS, la primera parte de este notebook se centra en configurar un agente con RAG. Si lo deseás, podés avanzar directamente a la sección **Integrando RAGAS** para ver el uso de RAGAS.\n",
        "\n",
        "Para comenzar, instalemos los requisitos previos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain chromadb ragas datasets python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VeosBMaIDs52"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Cargar variables del archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# Verifica si se cargó correctamente (opcional)\n",
        "assert os.getenv(\"OPENAI_API_KEY\") is not None, \"Falta la clave de OpenAI\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpKfZkUYzQhB"
      },
      "source": [
        "## Fuente de conocimientos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDTQoxcNzUa8"
      },
      "source": [
        "Lo primero que necesitamos para un agente que utilice RAG es una fuente desde la cual recuperar conocimiento. En este caso, utilizaremos la versión 2 del dataset **AI ArXiv**, disponible en Hugging Face Datasets en [`jamescalam/ai-arxiv2-chunks`](https://huggingface.co/datasets/jamescalam/ai-arxiv2-chunks).\n",
        "\n",
        "_Nota: estamos usando el dataset ya preprocesado en fragmentos (prechunked). Para acceder a la versión sin procesar, consultá [`jamescalam/ai-arxiv2`](https://huggingface.co/datasets/jamescalam/ai-arxiv2)._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297,
          "referenced_widgets": [
            "4b77ad5351bb4025be38d174ee9288a2",
            "e0be261c59d24b6188440a59391c16b8",
            "d450832f530c430cbb8040a1febb8d67",
            "b4ddf1bfdfe94e879c68b9ecfeae423e",
            "1963e604f6f84d1394ef0f5329bfed83",
            "83dfc2c36bd54ec6867d76a4f23e6726",
            "9dcbadd1b0864b99bb8ddc7631d99de0",
            "a06cec752c844242812abaec6aab0ad0",
            "6e374f4315674d6aaa0ff137aa50cac5",
            "9d5b412671284b85ad87d0fc93cc517b",
            "0e1562bc68a2493f8714fe490b21f0a0",
            "fc58156677c04c30a8d0c468c2a9421b",
            "2c6bcde588684c74b31e0ba64652bf10",
            "f923f5f768d548f88baa37bade011d7b",
            "e952debe03384be983daaff7306a2a1e",
            "d8e21208bded4906967180f28557b046",
            "aab214cee4414cfb95a525ea11635d40",
            "aaa3db9ab46a439a82ab82e308820df4",
            "c467e357ea5742efaf9d07c67b33bab6",
            "e503d6bc4517471fa75e4cd62d9081ac",
            "8482c2ffa17444f88d6c46163df3499a",
            "762eea88a5824b3ebe599eb82c48788a"
          ]
        },
        "id": "U9gpYFnzbFKm",
        "outputId": "4d65517e-e5b9-4182-b0cb-9a5cb60f2438"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aguro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"jamescalam/ai-arxiv2-chunks\", split=\"train[:20000]\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bP7ZW-ybFKm",
        "outputId": "f60b1622-3462-4d0d-9845-c333667eab2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'doi': '2401.09350',\n",
              " 'chunk-id': 1,\n",
              " 'chunk': 'These neural networks and their training algorithms may be complex, and the scope of their impact broad and wide, but nonetheless they are simply functions in a high-dimensional space. A trained neural network takes a vector as input, crunches and transforms it in various ways, and produces another vector, often in some other space. An image may thereby be turned into a vector, a song into a sequence of vectors, and a social network as a structured collection of vectors. It seems as though much of human knowledge, or at least what is expressed as text, audio, image, and video, has a vector representation in one form or another.\\nIt should be noted that representing data as vectors is not unique to neural networks and deep learning. In fact, long before learnt vector representations of pieces of dataâ\\x80\\x94what is commonly known as â\\x80\\x9cembeddingsâ\\x80\\x9dâ\\x80\\x94came along, data was often encoded as hand-crafted feature vectors. Each feature quanti- fied into continuous or discrete values some facet of the data that was deemed relevant to a particular task (such as classification or regression). Vectors of that form, too, reflect our understanding of a real-world object or concept.',\n",
              " 'id': '2401.09350#1',\n",
              " 'title': 'Foundations of Vector Retrieval',\n",
              " 'summary': 'Vectors are universal mathematical objects that can represent text, images,\\nspeech, or a mix of these data modalities. That happens regardless of whether\\ndata is represented by hand-crafted features or learnt embeddings. Collect a\\nlarge enough quantity of such vectors and the question of retrieval becomes\\nurgently relevant: Finding vectors that are more similar to a query vector.\\nThis monograph is concerned with the question above and covers fundamental\\nconcepts along with advanced data structures and algorithms for vector\\nretrieval. In doing so, it recaps this fascinating topic and lowers barriers of\\nentry into this rich area of research.',\n",
              " 'source': 'http://arxiv.org/pdf/2401.09350',\n",
              " 'authors': 'Sebastian Bruch',\n",
              " 'categories': 'cs.DS, cs.IR',\n",
              " 'comment': None,\n",
              " 'journal_ref': None,\n",
              " 'primary_category': 'cs.DS',\n",
              " 'published': '20240117',\n",
              " 'updated': '20240117',\n",
              " 'references': []}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX6NdQhgbFKn"
      },
      "source": [
        "## Construcción de la Base de Conocimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCbqQl_bFKn"
      },
      "source": [
        "Para construir nuestra base de conocimiento necesitamos _dos cosas_:\n",
        "\n",
        "1. **Embeddings**: en este ejemplo utilizamos `OpenAIEmbeddings` con el modelo `text-embedding-3-small` de OpenAI.\n",
        "2. **Una base de datos vectorial**, donde almacenamos los embeddings y realizamos consultas. Usamos **Chroma** como almacenamiento persistente local.\n",
        "\n",
        "Además, usamos la librería `tiktoken` para calcular la cantidad de tokens total y estimar el costo de los embeddings antes de ejecutarlo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens totales: 5027645\n",
            "Costo estimado: $0.100553 USD\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Cargar dataset (ajustá el split según lo que uses)\n",
        "data = dataset.to_pandas()\n",
        "\n",
        "# Lista de textos\n",
        "texts = data[\"chunk\"].tolist()\n",
        "\n",
        "# Calcular tokens por texto con tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
        "token_counts = [len(enc.encode(text, disallowed_special=())) for text in texts]\n",
        "\n",
        "# Total\n",
        "total_tokens = sum(token_counts)\n",
        "cost_per_1M = 0.02\n",
        "estimated_cost_usd = (total_tokens / 1000000) * cost_per_1M\n",
        "\n",
        "print(f\"Tokens totales: {total_tokens}\")\n",
        "print(f\"Costo estimado: ${estimated_cost_usd:.6f} USD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "j0N7EcJibFKo"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Chroma(persist_directory=\"chroma_db\", embedding_function=embedding_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZUn2lu7bFKp"
      },
      "source": [
        "### Poblando nuestra base de datos vectorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeVD6d0sbFKp"
      },
      "source": [
        "Ahora nuestra base de conocimiento está lista para ser poblada con los datos.\n",
        "\n",
        "Vamos a agregar los documentos a la base de datos vectorial (`Chroma`), que internamente calculará los embeddings usando el modelo `text-embedding-3-small` de OpenAI.\n",
        "\n",
        "Además, incluimos los metadatos de cada documento, como el título y la fuente, para facilitar búsquedas y trazabilidad futuras.\n",
        "\n",
        "`Nota: Antes de insertar los textos en la base vectorial, aplicamos una pequeña función de limpieza. Esta función remueve el token especial <|endoftext|>, que a veces aparece al final de ciertos textos generados por modelos de lenguaje. Este token no aporta información semántica útil y puede interferir en el proceso de embedding o recuperación si se deja tal cual.\n",
        "\n",
        "Al reemplazarlo por un espacio y eliminar espacios sobrantes con .strip(), aseguramos que los textos estén limpios y normalizados antes de ser embebidos y almacenados.`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b6fc6cb9ba394e228296a79d9bb81f2b",
            "9b6aa18bad374ab7a124092ddc5f7b22",
            "e1df49b9f89049cb93568cd2ee55afbf",
            "cfcd47466da84513afcd001bc025af0a",
            "ac69b8f2af604a21bac6fe4d9a62b084",
            "604b6cbd494646a09c28a932dae4f9f1",
            "8b787653fbb44649935b49b088045177",
            "f854ce0db34a4945afaafd7eaf787081",
            "766a0f2c7916452389943f94549ead67",
            "6e0fd11b754a4625b63aa7330fc96032",
            "8c2593763f4d49239c7bab4c9117cb2b"
          ]
        },
        "id": "hb00VSTqbFKp",
        "outputId": "6ee3ba32-2611-4d0e-b346-8968b6b154f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200/200 [00:04<00:00, 47.10it/s]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 100\n",
        "\n",
        "def clean_special_tokens(text):\n",
        "    return text.replace(\"<|endoftext|>\", \" \").strip()\n",
        "\n",
        "# Ver textos ya cargados\n",
        "existing_docs = vectorstore.get()\n",
        "existing_texts = set(existing_docs[\"documents\"])\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i + batch_size)\n",
        "    batch = data.iloc[i:i_end]\n",
        "\n",
        "    texts = batch[\"chunk\"].tolist()\n",
        "    metadatas = batch[[\"source\", \"title\", \"chunk\"]].to_dict(orient=\"records\")\n",
        "\n",
        "    # Filtrar textos que no están ya en la base\n",
        "    new_texts_and_metas = [\n",
        "        (clean_special_tokens(t), m) for t, m in zip(texts, metadatas) if t not in existing_texts\n",
        "    ]\n",
        "\n",
        "    if new_texts_and_metas:\n",
        "        new_texts, new_metas = zip(*new_texts_and_metas)\n",
        "        vectorstore.add_texts(texts=list(new_texts), metadatas=list(new_metas))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6VVT3X_EMDO"
      },
      "source": [
        "Creamos una herramienta (tool) que pueda utilizar nuestro agente para buscar papers de ArXiv:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "X9J5jHKcEQz6"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import tool\n",
        "\n",
        "@tool\n",
        "def arxiv_search(query: str) -> str:\n",
        "    \"\"\"Use this tool when answering questions about AI, machine learning, data\n",
        "    science, or other technical questions that may be answered using arXiv papers.\n",
        "    \"\"\"\n",
        "    # Buscar en Chroma\n",
        "    results = vectorstore.similarity_search(query, k=5)\n",
        "    \n",
        "    # Convertir resultados en string legible\n",
        "    results_str = \"\\n---\\n\".join([doc.page_content for doc in results])\n",
        "    \n",
        "    return results_str\n",
        "\n",
        "tools = [arxiv_search]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN7d_4r-JMPW"
      },
      "source": [
        "Cuando el agente use esta tool, se ejecutará de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq4H-2RpI1U3",
        "outputId": "72b427a8-6807-409e-eeb8-46cd95e0ab45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ethical Considerations and Limitations (Section 5.2) Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2âs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their speciï¬c applications of the model. Please see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-user-guide\n",
            "Table 52: Model card for Llama 2.\n",
            "77\n",
            "---\n",
            "# GenAI, Meta\n",
            "# Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed- source models. We provide a detailed description of our approach to ï¬ne-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\n",
            "âEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com â Second author\n",
            "Contributions for all the authors can be found in Section A.1.\n",
            "# Contents\n",
            "# 1 Introduction\n",
            "---\n",
            "In this work, we develop and release Llama 2, a family of pretrained and ï¬ne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-speciï¬c data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our ï¬ne-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce ï¬ne-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.\n",
            "3\n",
            "---\n",
            "We are releasing the following models to the general public for research and commercial useâ¡:\n",
            "1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§\n",
            "2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.\n",
            "---\n",
            "World Knowledge. We evaluate the Llama 2 model together with other open-source models on the Natu- ralQuestions and TriviaQA benchmarks (Table 22).\n",
            "Reading Comprehension In Table 23 we report zero-shot and few-shot results on SQUAD and zero-shot and one-shot experiments on QUAC. Here Llama 2 performs best on all evaluation settings and models except the QUAC 0-shot where Llama 1 30B performs slightly better.\n",
            "In Table 24, we present ï¬ne-grained results from the English part of the AGI Eval (Zhong et al., Exams. 2023) benchmark. AGI Eval is a collection of standardized exams in diï¬erent subjects.\n",
            "48\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    arxiv_search.run(tool_input={\"query\": \"can you tell me about llama 2?\"})\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUvJOqrNhYIh"
      },
      "source": [
        "## Definiento el Agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s45dwd78hbvk"
      },
      "source": [
        "Definimos el agente ReAct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntuT7UuXeMz0",
        "outputId": "5fbd3eb2-1bf0-4023-a5a8-5d010937c7cb"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Definimos el historial de conversación.\n",
        "memory = ConversationBufferMemory(\n",
        "                                memory_key=\"chat_history\",\n",
        "                                return_messages=True,\n",
        "                                output_key=\"output\"\n",
        "                                )\n",
        "\n",
        "# Definí tus herramientas (ejemplo)\n",
        "tools = [\n",
        "    Tool(name=\"arxiv_search\", func=arxiv_search, description=\"Buscar papers en ArXiv\"),\n",
        "    # agregá más tools si tenés\n",
        "]\n",
        "\n",
        "# Usá GPT‑4.1 nano como LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4.1-nano\", temperature=0)\n",
        "\n",
        "# Inicializá el agente con RETRIEVE‑THINK‑ACT (ReAct)\n",
        "agent_executor = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    return_intermediate_steps=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRCtHauRlkLc"
      },
      "source": [
        "Definimos la función 'chat' para facilitar el manejo de las salidas, contextos y pasos intermedios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Aqp20qloj7",
        "outputId": "705a299d-e2f8-441e-8ab4-525bd0ad0a88"
      },
      "outputs": [],
      "source": [
        "def chat(text: str):\n",
        "    # Ejecutamos el agente\n",
        "    result = agent_executor.invoke({\"input\": text})\n",
        "    \n",
        "    # Respuesta final del agente\n",
        "    output = result[\"output\"]\n",
        "    \n",
        "    # Recopilamos los pasos intermedios (herramienta + observación)\n",
        "    steps = result.get(\"intermediate_steps\", [])\n",
        "    \n",
        "    # Extraemos contextos: herramienta -> respuesta\n",
        "    contexts = [obs for (action, obs) in steps]\n",
        "    \n",
        "    # Imprimimos para lograr mayor claridad\n",
        "    print(f\"💬 Usuario: {text}\")\n",
        "    print(f\"🤖 Respuesta: {output}\")\n",
        "    if contexts:\n",
        "        print(\"🧠 Contextos recuperados:\")\n",
        "        for idx, c in enumerate(contexts, start=1):\n",
        "            print(f\"  {idx}. {c[:200].replace(chr(10), ' ')}{'...' if len(c)>200 else ''}\")\n",
        "    else:\n",
        "        print(\"⚠️ No se recuperaron contextos.\")\n",
        "    \n",
        "    # Retornamos todo para evaluación posterior\n",
        "    return {\"output\": output, \"contexts\": contexts, \"intermediate_steps\": steps}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ_PH7YcA_f2",
        "outputId": "bd2bff11-f71e-46cf-bd9d-877ae8a2f3c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"arxiv_search\",\n",
            "    \"action_input\": \"Llama 2 language model\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mWe are releasing the following models to the general public for research and commercial useâ¡:\n",
            "1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§\n",
            "2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well.\n",
            "---\n",
            "# GenAI, Meta\n",
            "# Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed- source models. We provide a detailed description of our approach to ï¬ne-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\n",
            "âEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com â Second author\n",
            "Contributions for all the authors can be found in Section A.1.\n",
            "# Contents\n",
            "# 1 Introduction\n",
            "---\n",
            "Ethical Considerations and Limitations (Section 5.2) Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2âs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their speciï¬c applications of the model. Please see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-user-guide\n",
            "Table 52: Model card for Llama 2.\n",
            "77\n",
            "---\n",
            "Model Developers Meta AI Variations Llama 2 comes in a range of parameter sizesâ7B, 13B, and 70Bâas well as pretrained and ï¬ne-tuned variations. Input Models input text only. Output Models generate text only. Model Architecture Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised ï¬ne-tuning (SFT) and reinforce- ment learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. Model Dates Llama 2 was trained between January 2023 and July 2023. Status This is a static model trained on an oï¬ine dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. License Where to send com- ments A custom commercial models-and-libraries/llama-downloads/ Instructions on how to provide feedback or comments on the model can be found in the model README, or by opening an issue in the GitHub repository (https://github.com/facebookresearch/llama/). license is available at: ai.meta.com/resources/ Intended Use Intended Use Cases Llama 2 is intended for commercial and research use in\n",
            "---\n",
            "Furthermore, our initial version of Llama 2-Chat predominantly concentrated on English-language data. While our experimental observations suggest the model has garnered some proï¬ciency in other languages, its proï¬ciency is limited, due primarily to the limited amount of pretraining data available in non-English languages (as documented in Table 10). Consequently, the modelâs performance in languages other than English remains fragile and should be used with caution.\n",
            "Like other LLMs, Llama 2 may generate harmful, oï¬ensive, or biased content due to its training on publicly available online datasets. We attempted to mitigate this via ï¬ne-tuning, but some issues may remain, particularly for languages other than English where publicly available datasets were not available. We will continue to ï¬ne-tune and release updated versions in the future as we progress on addressing these issues.\n",
            "â¡â¡https://openai.com/blog/chatgpt-plugins\n",
            "34\n",
            "Not everyone who uses AI models has good intentions, and conversational AI agents could potentially be used for nefarious purposes such as generating misinformation or retrieving information about topics like bioterrorism or cybercrime. We have, however, made eï¬orts to tune the models to avoid these topics and diminish any capabilities they might have oï¬ered for those use cases.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Llama 2 is a collection of large language models developed by Meta, available in sizes of 7B, 13B, and 70B parameters. It includes both pretrained and fine-tuned versions, with the fine-tuned Llama 2-Chat optimized for dialogue. The models are designed for research and commercial use, with safety and helpfulness considerations in mind. They are trained on publicly available data, primarily in English, and may have limited proficiency in other languages. Users should perform safety testing before deployment, as the models can produce inaccurate or biased outputs. More information can be found in the model's documentation and responsible use guidelines.\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "💬 Usuario: Can you tell me about llama 2?\n",
            "🤖 Respuesta: Llama 2 is a collection of large language models developed by Meta, available in sizes of 7B, 13B, and 70B parameters. It includes both pretrained and fine-tuned versions, with the fine-tuned Llama 2-Chat optimized for dialogue. The models are designed for research and commercial use, with safety and helpfulness considerations in mind. They are trained on publicly available data, primarily in English, and may have limited proficiency in other languages. Users should perform safety testing before deployment, as the models can produce inaccurate or biased outputs. More information can be found in the model's documentation and responsible use guidelines.\n",
            "🧠 Contextos recuperados:\n",
            "  1. We are releasing the following models to the general public for research and commercial useâ¡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also incr...\n",
            "Llama 2 is a collection of large language models developed by Meta, available in sizes of 7B, 13B, and 70B parameters. It includes both pretrained and fine-tuned versions, with the fine-tuned Llama 2-Chat optimized for dialogue. The models are designed for research and commercial use, with safety and helpfulness considerations in mind. They are trained on publicly available data, primarily in English, and may have limited proficiency in other languages. Users should perform safety testing before deployment, as the models can produce inaccurate or biased outputs. More information can be found in the model's documentation and responsible use guidelines.\n"
          ]
        }
      ],
      "source": [
        "print(chat(\"Can you tell me about llama 2?\")[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p8m4Gc5w1OX"
      },
      "source": [
        "We can ask follow up questions that miss key information but thanks to the conversational history the LLM understands the context and uses that to adjust the search query.\n",
        "\n",
        "_Note: if missing `\"chat_history\"` parameter from the `agent` definition you will likely notice a lack of context in the search term, and in some cases this lack of good information can trigger a `ValueError` during output parsing._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XJ_3JIgBDRl",
        "outputId": "23ba5d06-5b63-433e-c8d5-6ea4a94f874d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"arxiv_search\",\n",
            "    \"action_input\": \"Llama 2 red teaming safety evaluation\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m. . . . . . . . . . . . . . . 3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "---\n",
            "In addition to red teaming sessions, we ran a quantitative evaluation on risk from generating malicious code by scoring Code Llamaâs responses to ChatGPTâs (GPT3.5 Turbo) with LLAMAv2 70Bâs safety reward model. For this second quantitative evaluation, we selected prompts that the red teamers generated specifically attempting to solicit malicious code (even though the red teaming included consideration of a broad set of safety risks). These prompts were a mix of clear intent and slightly obfuscated intentions (see some examples in Figure 16. We show a KDE plot of the distribution of the safety score for all models in Figure 7). We observe that Code Llama tends to answer with safer responses; the distribution of safety scores for Code Llama has more weight in the safer part of the range.\n",
            "False refusals. LLMs that are too safe can have a tendency to over-refuse valid claims similar to what was reported after the release of Llama 2. We specifically asked red teamers to test for this behavior. They found some limited evidence of false refusals (when not using a system preprompt). False refusals could also\n",
            "16\n",
            "---\n",
            "# 4.3 Red Teaming\n",
            "Given how broad the capabilities of LLMs are and how varied their training data is, it is insuï¬cient to identify risks solely via ex post facto usage and analysis. Rather, as has been done for other LLMs, we performed various kinds of proactive risk identiï¬cation, colloquially called âred teaming,â based on the term commonly used within computer security. This kind of granular analysis is very important because safety is a long-tail issue, in which even very infrequent edge cases can cause noticeable problems. Even if quantitative scores report good results, these types of qualitative insights allow us to recognize and target speciï¬c patterns in a more comprehensive way.\n",
            "We conducted a series of red teaming with various groups of internal employees, contract workers, and external vendors. These teams included over 350 people, including domain experts in cybersecurity, elec- tion fraud, social media misinformation, legal, policy, civil rights, ethics, software engineering, machine learning, responsible AI, and creative writing. They also included individuals representative of a variety of socioeconomic, gender, ethnicity, and racial demographics.\n",
            "28\n",
            "---\n",
            "Deploying systems at scale with weaker safety guardrails can be exceedingly detrimental and poten- tially enticing for adversaries. While generic interactions between users and these deployed systems might seem safe, the necessity for innovative and systematic red teaming is crucial to identify loop- holes within these systems and play an essential role in enhancing their robustness. As a result, it becomes essential to systematically test and provide assurances of the systemâs robustness before deploying it at a large scale. This is particularly important given the rapid ubiquity of LLMs, their increasing size, and the vast amount of data theyâve processed, which has made conducting safety evaluations even more challenging.\n",
            "Existing safety evaluations, for instance, Shaikh et al. [16] performs a Chain-of-Thought (CoT)- based assessments with prompts constituting of harmful questions with âLetâs think step by step.â as its suffix. Even though effective, we observed that LLMs we study in this work can easily identify such questions, e.g., âHow do I build a bomb?\", and show a high refusal rate (fraction of time model identifies a harmful question and does not provide a harmful answer) by raising ethical warnings, straightaway refusing to answer or answer a related but ethical query (Table 3).\n",
            "---\n",
            "28\n",
            "The red teamers probed our models across a wide range of risk categories (such as criminal planning, human traï¬cking, regulated or controlled substances, sexually explicit content, unqualiï¬ed health or ï¬nancial advice, privacy violations, and more), as well as diï¬erent attack vectors (such as hypothetical questions, malformed/misspelled inputs, or extended dialogues). Additionally, we conducted speciï¬c tests to determine the capabilities of our models to facilitate the production of weapons (e.g. nuclear, biological, chemical, and cyber); ï¬ndings on these topics were marginal and were mitigated. Nonetheless, we will continue our red teaming eï¬orts in this front.\n",
            "To date, all of our red teaming eï¬orts have targeted model outputs in English, but have crucially included non-English prompts and dialogue contexts, as that is a well-known attack vector. In all exercises, participants were given risk category deï¬nitions and were shown just a handful of examples of risky interactions with an LLM. After that, each participant was part of a subteam focused on a particular category of risk or attack vector. After creating each dialogue, the red team participant would annotate various attributes, including risk areas and degree of risk, as captured by a 5-point Likert scale.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Based on the information available, extensive red teaming was conducted as part of the safety evaluation process for Llama 2. Over 350 internal and external experts participated in proactive risk identification, probing the model across various risk categories and attack vectors, including non-English prompts. The red teaming aimed to identify vulnerabilities and improve robustness before deployment, covering areas such as malicious content, false refusals, and safety in responses. This comprehensive approach highlights the importance placed on safety and robustness in the development of Llama 2.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "💬 Usuario: was any red teaming done?\n",
            "🤖 Respuesta: Based on the information available, extensive red teaming was conducted as part of the safety evaluation process for Llama 2. Over 350 internal and external experts participated in proactive risk identification, probing the model across various risk categories and attack vectors, including non-English prompts. The red teaming aimed to identify vulnerabilities and improve robustness before deployment, covering areas such as malicious content, false refusals, and safety in responses. This comprehensive approach highlights the importance placed on safety and robustness in the development of Llama 2.\n",
            "🧠 Contextos recuperados:\n",
            "  1. . . . . . . . . . . . . . . . 3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Safety in Pretraining . . . . . . . . . . . . . . . . . . . ....\n",
            "Based on the information available, extensive red teaming was conducted as part of the safety evaluation process for Llama 2. Over 350 internal and external experts participated in proactive risk identification, probing the model across various risk categories and attack vectors, including non-English prompts. The red teaming aimed to identify vulnerabilities and improve robustness before deployment, covering areas such as malicious content, false refusals, and safety in responses. This comprehensive approach highlights the importance placed on safety and robustness in the development of Llama 2.\n"
          ]
        }
      ],
      "source": [
        "out = chat(\"was any red teaming done?\")\n",
        "print(out[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bI9czPtWnl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNHpkMPoDKEV"
      },
      "source": [
        "## Integrando RAGAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuyybSe5FlZg"
      },
      "source": [
        "Para integrar la evaluación con RAGAS en este pipeline, necesitamos algunas cosas: los contextos recuperados y la salida generada.\n",
        "\n",
        "Veamos que devuelve `out`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPJkvJh1DJSD",
        "outputId": "ea3f9902-ca9b-413e-d659-0eabdd097d4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'output': 'Based on the information available, extensive red teaming was conducted as part of the safety evaluation process for Llama 2. Over 350 internal and external experts participated in proactive risk identification, probing the model across various risk categories and attack vectors, including non-English prompts. The red teaming aimed to identify vulnerabilities and improve robustness before deployment, covering areas such as malicious content, false refusals, and safety in responses. This comprehensive approach highlights the importance placed on safety and robustness in the development of Llama 2.',\n",
              " 'contexts': ['. . . . . . . . . . . . . . . 3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . .\\n---\\nIn addition to red teaming sessions, we ran a quantitative evaluation on risk from generating malicious code by scoring Code Llamaâ\\x80\\x99s responses to ChatGPTâ\\x80\\x99s (GPT3.5 Turbo) with LLAMAv2 70Bâ\\x80\\x99s safety reward model. For this second quantitative evaluation, we selected prompts that the red teamers generated specifically attempting to solicit malicious code (even though the red teaming included consideration of a broad set of safety risks). These prompts were a mix of clear intent and slightly obfuscated intentions (see some examples in Figure 16. We show a KDE plot of the distribution of the safety score for all models in Figure 7). We observe that Code Llama tends to answer with safer responses; the distribution of safety scores for Code Llama has more weight in the safer part of the range.\\nFalse refusals. LLMs that are too safe can have a tendency to over-refuse valid claims similar to what was reported after the release of Llama 2. We specifically asked red teamers to test for this behavior. They found some limited evidence of false refusals (when not using a system preprompt). False refusals could also\\n16\\n---\\n# 4.3 Red Teaming\\nGiven how broad the capabilities of LLMs are and how varied their training data is, it is insuï¬\\x83cient to identify risks solely via ex post facto usage and analysis. Rather, as has been done for other LLMs, we performed various kinds of proactive risk identiï¬\\x81cation, colloquially called â\\x80\\x9cred teaming,â\\x80\\x9c based on the term commonly used within computer security. This kind of granular analysis is very important because safety is a long-tail issue, in which even very infrequent edge cases can cause noticeable problems. Even if quantitative scores report good results, these types of qualitative insights allow us to recognize and target speciï¬\\x81c patterns in a more comprehensive way.\\nWe conducted a series of red teaming with various groups of internal employees, contract workers, and external vendors. These teams included over 350 people, including domain experts in cybersecurity, elec- tion fraud, social media misinformation, legal, policy, civil rights, ethics, software engineering, machine learning, responsible AI, and creative writing. They also included individuals representative of a variety of socioeconomic, gender, ethnicity, and racial demographics.\\n28\\n---\\nDeploying systems at scale with weaker safety guardrails can be exceedingly detrimental and poten- tially enticing for adversaries. While generic interactions between users and these deployed systems might seem safe, the necessity for innovative and systematic red teaming is crucial to identify loop- holes within these systems and play an essential role in enhancing their robustness. As a result, it becomes essential to systematically test and provide assurances of the systemâ\\x80\\x99s robustness before deploying it at a large scale. This is particularly important given the rapid ubiquity of LLMs, their increasing size, and the vast amount of data theyâ\\x80\\x99ve processed, which has made conducting safety evaluations even more challenging.\\nExisting safety evaluations, for instance, Shaikh et al. [16] performs a Chain-of-Thought (CoT)- based assessments with prompts constituting of harmful questions with â\\x80\\x9cLetâ\\x80\\x99s think step by step.â\\x80\\x9d as its suffix. Even though effective, we observed that LLMs we study in this work can easily identify such questions, e.g., â\\x80\\x9cHow do I build a bomb?\", and show a high refusal rate (fraction of time model identifies a harmful question and does not provide a harmful answer) by raising ethical warnings, straightaway refusing to answer or answer a related but ethical query (Table 3).\\n---\\n28\\nThe red teamers probed our models across a wide range of risk categories (such as criminal planning, human traï¬\\x83cking, regulated or controlled substances, sexually explicit content, unqualiï¬\\x81ed health or ï¬\\x81nancial advice, privacy violations, and more), as well as diï¬\\x80erent attack vectors (such as hypothetical questions, malformed/misspelled inputs, or extended dialogues). Additionally, we conducted speciï¬\\x81c tests to determine the capabilities of our models to facilitate the production of weapons (e.g. nuclear, biological, chemical, and cyber); ï¬\\x81ndings on these topics were marginal and were mitigated. Nonetheless, we will continue our red teaming eï¬\\x80orts in this front.\\nTo date, all of our red teaming eï¬\\x80orts have targeted model outputs in English, but have crucially included non-English prompts and dialogue contexts, as that is a well-known attack vector. In all exercises, participants were given risk category deï¬\\x81nitions and were shown just a handful of examples of risky interactions with an LLM. After that, each participant was part of a subteam focused on a particular category of risk or attack vector. After creating each dialogue, the red team participant would annotate various attributes, including risk areas and degree of risk, as captured by a 5-point Likert scale.'],\n",
              " 'intermediate_steps': [(AgentAction(tool='arxiv_search', tool_input='Llama 2 red teaming safety evaluation', log='```json\\n{\\n    \"action\": \"arxiv_search\",\\n    \"action_input\": \"Llama 2 red teaming safety evaluation\"\\n}\\n```'),\n",
              "   '. . . . . . . . . . . . . . . 3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . .\\n---\\nIn addition to red teaming sessions, we ran a quantitative evaluation on risk from generating malicious code by scoring Code Llamaâ\\x80\\x99s responses to ChatGPTâ\\x80\\x99s (GPT3.5 Turbo) with LLAMAv2 70Bâ\\x80\\x99s safety reward model. For this second quantitative evaluation, we selected prompts that the red teamers generated specifically attempting to solicit malicious code (even though the red teaming included consideration of a broad set of safety risks). These prompts were a mix of clear intent and slightly obfuscated intentions (see some examples in Figure 16. We show a KDE plot of the distribution of the safety score for all models in Figure 7). We observe that Code Llama tends to answer with safer responses; the distribution of safety scores for Code Llama has more weight in the safer part of the range.\\nFalse refusals. LLMs that are too safe can have a tendency to over-refuse valid claims similar to what was reported after the release of Llama 2. We specifically asked red teamers to test for this behavior. They found some limited evidence of false refusals (when not using a system preprompt). False refusals could also\\n16\\n---\\n# 4.3 Red Teaming\\nGiven how broad the capabilities of LLMs are and how varied their training data is, it is insuï¬\\x83cient to identify risks solely via ex post facto usage and analysis. Rather, as has been done for other LLMs, we performed various kinds of proactive risk identiï¬\\x81cation, colloquially called â\\x80\\x9cred teaming,â\\x80\\x9c based on the term commonly used within computer security. This kind of granular analysis is very important because safety is a long-tail issue, in which even very infrequent edge cases can cause noticeable problems. Even if quantitative scores report good results, these types of qualitative insights allow us to recognize and target speciï¬\\x81c patterns in a more comprehensive way.\\nWe conducted a series of red teaming with various groups of internal employees, contract workers, and external vendors. These teams included over 350 people, including domain experts in cybersecurity, elec- tion fraud, social media misinformation, legal, policy, civil rights, ethics, software engineering, machine learning, responsible AI, and creative writing. They also included individuals representative of a variety of socioeconomic, gender, ethnicity, and racial demographics.\\n28\\n---\\nDeploying systems at scale with weaker safety guardrails can be exceedingly detrimental and poten- tially enticing for adversaries. While generic interactions between users and these deployed systems might seem safe, the necessity for innovative and systematic red teaming is crucial to identify loop- holes within these systems and play an essential role in enhancing their robustness. As a result, it becomes essential to systematically test and provide assurances of the systemâ\\x80\\x99s robustness before deploying it at a large scale. This is particularly important given the rapid ubiquity of LLMs, their increasing size, and the vast amount of data theyâ\\x80\\x99ve processed, which has made conducting safety evaluations even more challenging.\\nExisting safety evaluations, for instance, Shaikh et al. [16] performs a Chain-of-Thought (CoT)- based assessments with prompts constituting of harmful questions with â\\x80\\x9cLetâ\\x80\\x99s think step by step.â\\x80\\x9d as its suffix. Even though effective, we observed that LLMs we study in this work can easily identify such questions, e.g., â\\x80\\x9cHow do I build a bomb?\", and show a high refusal rate (fraction of time model identifies a harmful question and does not provide a harmful answer) by raising ethical warnings, straightaway refusing to answer or answer a related but ethical query (Table 3).\\n---\\n28\\nThe red teamers probed our models across a wide range of risk categories (such as criminal planning, human traï¬\\x83cking, regulated or controlled substances, sexually explicit content, unqualiï¬\\x81ed health or ï¬\\x81nancial advice, privacy violations, and more), as well as diï¬\\x80erent attack vectors (such as hypothetical questions, malformed/misspelled inputs, or extended dialogues). Additionally, we conducted speciï¬\\x81c tests to determine the capabilities of our models to facilitate the production of weapons (e.g. nuclear, biological, chemical, and cyber); ï¬\\x81ndings on these topics were marginal and were mitigated. Nonetheless, we will continue our red teaming eï¬\\x80orts in this front.\\nTo date, all of our red teaming eï¬\\x80orts have targeted model outputs in English, but have crucially included non-English prompts and dialogue contexts, as that is a well-known attack vector. In all exercises, participants were given risk category deï¬\\x81nitions and were shown just a handful of examples of risky interactions with an LLM. After that, each participant was part of a subteam focused on a particular category of risk or attack vector. After creating each dialogue, the red team participant would annotate various attributes, including risk areas and degree of risk, as captured by a 5-point Likert scale.')]}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjFyIBvFLcIo"
      },
      "source": [
        "Al inicializar nuestro agente con `return_intermediate_steps=True`, estamos recibiendo automáticamente los pasos intermedios (tuplas de acción y observación) que el agente ejecuta para generar la respuesta final. Esos pasos incluyen la salida del `arxiv_search`, que podemos utilizar para evaluar la fase de recuperación de contexto en nuestro pipeline usando RAGAS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwCU9B1rOkZ9"
      },
      "source": [
        "Extraemos los contextos recuperados de esta forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He3I1jAGOnI9",
        "outputId": "ad4a854f-0381-4637-dd2a-f9459f07403c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ". . . . . . . . . . . . . . . 3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "---\n",
            "In addition to red teaming sessions, we ran a quantitative evaluation on risk from generating malicious code by scoring Code Llamaâs responses to ChatGPTâs (GPT3.5 Turbo) with LLAMAv2 70Bâs safety reward model. For this second quantitative evaluation, we selected prompts that the red teamers generated specifically attempting to solicit malicious code (even though the red teaming included consideration of a broad set of safety risks). These prompts were a mix of clear intent and slightly obfuscated intentions (see some examples in Figure 16. We show a KDE plot of the distribution of the safety score for all models in Figure 7). We observe that Code Llama tends to answer with safer responses; the distribution of safety scores for Code Llama has more weight in the safer part of the range.\n",
            "False refusals. LLMs that are too safe can have a tendency to over-refuse valid claims similar to what was reported after the release of Llama 2. We specifically asked red teamers to test for this behavior. They found some limited evidence of false refusals (when not using a system preprompt). False refusals could also\n",
            "16\n",
            "---\n",
            "# 4.3 Red Teaming\n",
            "Given how broad the capabilities of LLMs are and how varied their training data is, it is insuï¬cient to identify risks solely via ex post facto usage and analysis. Rather, as has been done for other LLMs, we performed various kinds of proactive risk identiï¬cation, colloquially called âred teaming,â based on the term commonly used within computer security. This kind of granular analysis is very important because safety is a long-tail issue, in which even very infrequent edge cases can cause noticeable problems. Even if quantitative scores report good results, these types of qualitative insights allow us to recognize and target speciï¬c patterns in a more comprehensive way.\n",
            "We conducted a series of red teaming with various groups of internal employees, contract workers, and external vendors. These teams included over 350 people, including domain experts in cybersecurity, elec- tion fraud, social media misinformation, legal, policy, civil rights, ethics, software engineering, machine learning, responsible AI, and creative writing. They also included individuals representative of a variety of socioeconomic, gender, ethnicity, and racial demographics.\n",
            "28\n",
            "---\n",
            "Deploying systems at scale with weaker safety guardrails can be exceedingly detrimental and poten- tially enticing for adversaries. While generic interactions between users and these deployed systems might seem safe, the necessity for innovative and systematic red teaming is crucial to identify loop- holes within these systems and play an essential role in enhancing their robustness. As a result, it becomes essential to systematically test and provide assurances of the systemâs robustness before deploying it at a large scale. This is particularly important given the rapid ubiquity of LLMs, their increasing size, and the vast amount of data theyâve processed, which has made conducting safety evaluations even more challenging.\n",
            "Existing safety evaluations, for instance, Shaikh et al. [16] performs a Chain-of-Thought (CoT)- based assessments with prompts constituting of harmful questions with âLetâs think step by step.â as its suffix. Even though effective, we observed that LLMs we study in this work can easily identify such questions, e.g., âHow do I build a bomb?\", and show a high refusal rate (fraction of time model identifies a harmful question and does not provide a harmful answer) by raising ethical warnings, straightaway refusing to answer or answer a related but ethical query (Table 3).\n",
            "---\n",
            "28\n",
            "The red teamers probed our models across a wide range of risk categories (such as criminal planning, human traï¬cking, regulated or controlled substances, sexually explicit content, unqualiï¬ed health or ï¬nancial advice, privacy violations, and more), as well as diï¬erent attack vectors (such as hypothetical questions, malformed/misspelled inputs, or extended dialogues). Additionally, we conducted speciï¬c tests to determine the capabilities of our models to facilitate the production of weapons (e.g. nuclear, biological, chemical, and cyber); ï¬ndings on these topics were marginal and were mitigated. Nonetheless, we will continue our red teaming eï¬orts in this front.\n",
            "To date, all of our red teaming eï¬orts have targeted model outputs in English, but have crucially included non-English prompts and dialogue contexts, as that is a well-known attack vector. In all exercises, participants were given risk category deï¬nitions and were shown just a handful of examples of risky interactions with an LLM. After that, each participant was part of a subteam focused on a particular category of risk or attack vector. After creating each dialogue, the red team participant would annotate various attributes, including risk areas and degree of risk, as captured by a 5-point Likert scale.\n"
          ]
        }
      ],
      "source": [
        "print(out[\"intermediate_steps\"][0][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuhMvGdBOnvu"
      },
      "source": [
        "## Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bczg8jREMwLw"
      },
      "source": [
        "Para evaluar con RAGAS necesitamos un conjunto de datos que incluya lo siguiente para cada ejemplo:\n",
        "\n",
        "`question`: la pregunta que se hace al agente,\n",
        "\n",
        "`contexts`: los contextos identiﬁcados (recuperados por la herramienta, como arxiv_search),\n",
        "\n",
        "`answer`: la respuesta generada por el agente,\n",
        "\n",
        "`ground_truth`: la respuesta correcta esperada (“ground truth”) para esa pregunta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152,
          "referenced_widgets": [
            "863e4257923b4cbebb58d743e7183c4f",
            "2224f5d0fc844021959c2af828e2f585",
            "0a78fba543d543cbac436e27e7aa71c1",
            "6ca105565301434093f5e23c7b7d66e9",
            "4cd10b8a91b1496b80c889a9d4acc590",
            "d0d606b8d2944a2892cd662126c4d809",
            "e17b7b79878e4e90b209b2970345575d",
            "8627743220b7417ca3485505da50c867",
            "ff165e061fab4b2da77cd58249d15ffe",
            "0926a4d528334d60b69b58f630973f75",
            "ba093531fbf84b82bc55574ebea2bec9",
            "42887df65980433ca54cb87a5861025a",
            "1f8f2db40c0e4aca9915d197581b0aae",
            "1a20c8704bdb43c7987db193e7dd9a8d",
            "c084482f54a344d88f41ca880476f17e",
            "425f05bbffe84e519550218f97bb4a5f",
            "67f7b83c028c43229697597e7e67e67d",
            "e6f41e1678d44d65af4f86d2f868f620",
            "3f6749cc0e13470bb83e2ed77c9f5cc7",
            "223831fe61f549a98b24070d824c461a",
            "02a95248d9b14b858170f69ce1b076e8",
            "0f93bb430a664d69acee7f01ee8405cd"
          ]
        },
        "id": "Ee-LezlRGSsu",
        "outputId": "7603f785-fc5f-4904-b14d-8d54dae8b965"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aguro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aguro\\.cache\\huggingface\\hub\\datasets--aurelio-ai--ai-arxiv2-ragas-mixtral. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Generating train split: 100%|██████████| 51/51 [00:00<00:00, 3262.11 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'ground_truth_context', 'ground_truth', 'question_type', 'episode_done'],\n",
              "    num_rows: 51\n",
              "})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ragas_data = load_dataset(\"aurelio-ai/ai-arxiv2-ragas-mixtral\", split=\"train\")\n",
        "ragas_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsSjPe0MNRi1",
        "outputId": "e579413b-f555-4a2d-bbeb-dba91723b716"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What is the impact of encoding the input prompt on inference speed in generative inference?',\n",
              " 'ground_truth_context': ['- This technique works particularly well when processing large batches of data, during train-\\ning Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al.\\n(2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded\\nfrom RAM.\\n- In turn, when doing interactive inference (e.g. as a chat assistants), offloading works\\nsignificantly slower than on-device inference.\\n- The generative inference workload consists of two phases: 1) encoding the input prompt and 2)\\ngenerating tokens conditioned on that prompt.\\n- The key difference between these two phases is that\\nprompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially\\n(token-by-token and layer-by-layer).\\n- In general, phase 1 works relatively well with existing Mixture-\\nof-Experts algorithms, since each layer can only be loaded once for the entire prompt.\\n- In turn, when\\ngenerating tokens, one must load layer once per each token generated.\\n- In practice, this means that\\ninference speed is limited by how fast one can fetch parameters from system memory.\\n- Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit\\nthese patterns to speed up inference time.\\n- As we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to\\nassign individual experts to distinct sub-tasks.\\n- However, this does not mean that the model uses the\\nsame expert over long stretches of tokens.\\n- Instead, some experts are active in short sequences of 2-4\\ntokens, while others are often used with “gaps”, as shown in Figure 1.\\n- To take advantage of this pattern, we can keep active experts in GPU memory as a “cache” for\\nfuture tokens.\\n- If the same experts are activated again in future, they will be available instantaneously.\\n- While LRU caching can reduce the average expert loading time, most of the inference time is still\\nspent waiting for the next expert to be loaded.\\n- The reason behind this is that, unlike with dense\\nmodels, MoE offloading cannot effectively overlap expert loading with computation.\\n- For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads\\nthe next transformer layer ahead of time, while the previous layer is still running.\\n- Unfortunately,\\nthis schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose\\nwhich experts to load just-in-time for computation.'],\n",
              " 'ground_truth': ['The encoding of the input prompt has an impact on inference speed in generative inference. During the encoding phase, prompt tokens are encoded in parallel, layer-by-layer, which works relatively well with existing Mixture-of-Experts algorithms. Each layer only needs to be loaded once for the entire prompt. However, during the generation phase, tokens are generated sequentially, and each token requires loading the layer once. This means that inference speed is limited by how fast the parameters can be fetched from system memory. The MoE model loads its experts in a pattern where some experts are active in short sequences of 2-4 tokens, while others are used with \"gaps\". To exploit this pattern and speed up inference time, active experts can be kept in GPU memory as a cache for future tokens. If the same experts are activated again in the future, they will be available instantaneously. However, even with caching, most of the inference time is still spent waiting for the next expert to be loaded because MoE offloading cannot effectively overlap expert loading with computation like dense models can.'],\n",
              " 'question_type': 'conditional',\n",
              " 'episode_done': False}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ragas_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU4QrqiPOF-w"
      },
      "source": [
        "Primero iteramos por las preguntas de nuestro dataset de evualiacion y le pedimos a nuestro agente responderlas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1d16e611b08f421299a126362d76cdc4",
            "6c46d4bb317b4d49ae70e1a0be044442",
            "14fe67648c60413f882f1d62efd2ffa7",
            "40bf35b701e2481b81de737c45090f7e",
            "26c5f9fcb6994125ad1a5077a9196aac",
            "53ad7c44377245fc9a4ce5112718c0c5",
            "060ce0847b474411ba1adc420ea0a6f0",
            "14c981775aae4cbb8b2deef325b7c88e",
            "9bf01738d11648d7bd9e3389d08da081",
            "ab7894a65e0e4ef8b0af1d413769e144",
            "84b6075b7c7748a18c1b526ef92c4c47"
          ]
        },
        "id": "rfK4Y2kGNUUD",
        "outputId": "a5359601-0846-4bcb-ee41-b0021a32efd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"arxiv_search\",\n",
            "    \"action_input\": \"impact of encoding input prompt on inference speed in generative models\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\n",
            "Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n",
            "4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n",
            "3\n",
            "Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture\n",
            "---\n",
            "els (Brown et al., 2020; Zhou et al., 2022; Muennighoff, 2022; Babe et al., 2023). To ensure fair evaluation we use the prompting format put forth by the respective authors of the models and a simple intuitive prompt for models without a canonical prompt (see Appendix N). However, this may put models without a canonical prompt recommendation (e.g. BLOOMZ, GPT-4) at a slight disadvantage. OCTOCODER and OCTOGEEX perform best when prompted using the same format we use during training (Figure 17) and we recommend always using this format at inference. (2) Processing: Models may accidentally impair otherwise correct code by e.g. including a natural language explanation in their output. We largely circumvent this issue through the use of strict stopping criteria and careful postprocessing (e.g. for GPT-4 we check if it has enclosed the code in backticks, and if so, extract only the inner part of the backticks discarding its explanations). (3) Execution: When executing code to compute pass@k, it is important that the generated code matches the installed programming language version. Models may inadvertently use expressions from\n",
            "---\n",
            "Figure 4: Modulating the input context length of the multi-document question answering example presented in Figure 2. Adding documents that do not contain the answer increases the length of the input context, but does not affect the desired output.\n",
            "Our experimental setup is similar to the needle- in-a-haystack experiments of Ivgi et al. (2023), who compare question answering performance when the relevant paragraph is placed (i) at the beginning of the input or (ii) a random position within the in- put. They find that encoder-decoder models have significantly higher performance when relevant in- formation is placed at the start of the input context. In contrast, we study finer-grained changes in the position of relevant information.\n",
            "# 2.2 Models\n",
            "We analyze several state-of-the-art open and closed language models. We use greedy decoding when generating outputs and leave exploration of other decoding methods to future work. We use a stan- dard set of prompts for each model (Figure 2).\n",
            "---\n",
            "For instance, after training the GPT-2 model, we observed that the resulting soft prompt merely replicates the prompt tokens used during initialization, essentially duplicating the manual prompt without additional learning. In con- trast, when decoding the virtual token vectors into words utilizing the LLaMA-7B and Vicuna-7B, we discovered that these models not only retain the initial prompt tokens but also acquire additional symbols and representations associated with relevant text, such as \"query,\" \"rewrite\", \"argument\", \"enhance\" and \"adding\", indicating parameters ð does learn task-specific knowledge.\n",
            "---\n",
            "Instruction tuning prompts\n",
            "A Alpaca prompt B. Our prompt .No prompt Below isan instruction that {Instruction} {instruction} dserbes a task, White a response that appropriately completes the uy âHt Response: {response} â#8 Inetruction: Ce) {instruction} âHt Response: {response}\n",
            "Figure 3: Comparative illustration of instruction tuning prompts. A. Alpaca prompt, a wrapper around instruction, B. only Alpaca sufï¬x, C. no prompt, the baseline\n",
            "Stage 1: Classifier training & LM prediction training instruction IFS Stage 2: Evaluation binary inference classifier LM response\n",
            "Figure 2: IFS training and evaluation pipeline\n",
            "The results presented in Table 6 show that vari- ants of both prompts are equally effective. If we compare it with the baseline (C), we see that for all models the improvement of IFS is in the range 0.5â0.6. It turns out that for Large Language Models (LLMs) a single prompt change can ef- fectively encourage models to follow instructions, reaching performance levels comparable to sev- eral publicly available instruct models. We did not test n-shot prompting, which can possibly further improve results.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:04<00:18,  4.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "💬 Usuario: What is the impact of encoding the input prompt on inference speed in generative inference?\n",
            "🤖 Respuesta: The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.\n",
            "🧠 Contextos recuperados:\n",
            "  1. The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt t...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"arxiv_search\",\n",
            "    \"action_input\": \"impact of token generation on inference speed in generative models\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3minference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021).\n",
            "---\n",
            "â¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from a token generation perspective. Instead of focusing on the number of tokens in the prompt, it takes into account the number of tokens generated. This is particularly significant because transformer-based generative LLMs produce content token-by-token, with each subsequent token relying on the gen- eration of preceding ones. Consequently, an increase in number of generated tokens leads to a corresponding increase in the computational cost, as each additional generated token implies another LLM forward inference. In fact, OpenAI applies a pricing structure wherein the cost for the number of generated tokens is twice that of the number of prompt tokens for their LLM APIs 1. This underscores the substantial impact that generated tokens can have on computational expenses.\n",
            "---\n",
            "The benefits are three-fold. First, with all generated solu- tions existing within a shared context, thereâs no need for in- dividual model queries for each solution evaluation. Second, while it may seem counterintuitive initially, isolated token or token group probabilities might not always yield meaning- ful choices. A simple illustration is found in Fig. 4. When evaluated independently, the second-most probable token for our inaugural number is â1âânot qualifying as prime. But, when generation remains unbroken, the derived sequence is correct. This incongruence points towards the restrictive na- ture of the Markov property in sequence modeling. Core to our perspective is the premise that for sequential tasks like algorithmic search, LLMs are more adept at generating en- tire sequences than intermittently pausing and re-initiating the token sampling process.\n",
            "---\n",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\n",
            "Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n",
            "4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n",
            "3\n",
            "Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture\n",
            "---\n",
            "# INTRODUCTION\n",
            "Large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI, 2023; Zheng et al., 2023) have shown exceptional performance in natural language processing and chatbot systems. However, the inference process of the state-of-the-art LLMs is slow, hindering their interactive use. For example, it takes 22 seconds for Claude (Anthropic, 2023) (accessed through Slack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on one NVIDIA A100 GPU) to answer the question in Fig. 1.\n",
            "We conclude three major causes of LLMsâ slow inference: (1) A large model size requires a large amount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT- 3 take 350GB memory, which means at least 5Ã80GB A100 GPUs are needed to keep the model in GPU memory. Even with enough GPUs, the heavy memory access and computation slow down the inference. (2) The attention operation in the prevailing transformer architecture is I/O bounded and has a quadratic memory and computation complexity in sequence length. (3) The sequential decoding approach in inference generates tokens one by one. This approach introduces a significant\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:09<00:13,  4.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "💬 Usuario: How does generating tokens affect the inference speed in generative inference?\n",
            "🤖 Respuesta: Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.\n",
            "🧠 Contextos recuperados:\n",
            "  1. inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"arxiv_search\",\n",
            "    \"action_input\": \"Mixtral 8x7B architecture differences from Mistral 7B feedforward blocks and inference parameters\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAbstract\n",
            "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\n",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n",
            "---\n",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n",
            "# 2 Architectural details\n",
            "Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value\n",
            "---\n",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
            "70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\n",
            "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\n",
            "---\n",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\n",
            "Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\n",
            "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:13<00:08,  4.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "💬 Usuario: How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?\n",
            "🤖 Respuesta: Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.\n",
            "🧠 Contextos recuperados:\n",
            "  1. Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"arxiv_search\",\n",
            "    \"action_input\": \"offloading A100 server MoE language models\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\n",
            "In this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\n",
            "we observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\n",
            "â¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2\n",
            "---\n",
            "Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y. Zero-offload: Democratizing billion-scale model training. CoRR, abs/2101.06840, 2021. URL https://arxiv.org/abs/2101.06840.\n",
            "Scao, T. L., Fan, A., Akiki, C., Pavlick, E., IliÂ´c, S., Hesslow, D., CastagnÃ©, R., Luccioni, A. S., Yvon, F., GallÃ©, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n",
            "Shahbaba, B. and Neal, R. Nonlinear models using dirichlet process mixtures. Journal of Machine Learning Research, 10(Aug):1829â1850, 2009.\n",
            "---\n",
            "# 5 Conclusion and Future Work\n",
            "In this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n",
            "7\n",
            "and explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\n",
            "Our method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n",
            "# Acknowledgements\n",
            "Authors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n",
            "# References\n",
            "Aminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.\n",
            "---\n",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).\n",
            "---\n",
            "Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Min- jia Zhang, Dong Li, and Yuxiong He. {ZeRO-Offload}: Democratizing {Billion-Scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551â564, 2021.\n",
            "13\n",
            "# Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\n",
            "Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Ric- cardo Marin, and Emanuele Rodol`a. Accelerating transformer inference for translation via paral- lel decoding. In acl, 2023.\n",
            "Timo Schick, Jane Dwivedi-Yu, Roberto Dess`Ä±, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n",
            "SenseTime. Lightllm. https://github.com/ModelTC/lightllm, 2023a. Accessed: 2023-09-26.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [00:17<00:04,  4.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "💬 Usuario: When is offloading used on the A100 server for accelerating MoE-based language models?\n",
            "🤖 Respuesta: Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.\n",
            "🧠 Contextos recuperados:\n",
            "  1. 1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory. In this work, we systematically develop techniques for runni...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"arxiv_search\",\n",
            "    \"action_input\": \"Mixtral 8x7B vs Llama 2 70B code benchmarks\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDetailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
            "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "3\n",
            "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\n",
            "---\n",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
            "70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\n",
            "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\n",
            "---\n",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "4\n",
            "---\n",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\n",
            "Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\n",
            "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:20<00:00,  4.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "💬 Usuario: How does Mixtral compare to Llama 2 70B in code benchmarks?\n",
            "🤖 Respuesta: Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.\n",
            "🧠 Contextos recuperados:\n",
            "  1. Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categorie...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"question\": [],\n",
        "    \"contexts\": [],\n",
        "    \"answer\": [],\n",
        "    \"ground_truth\": []\n",
        "})\n",
        "\n",
        "limit = 5\n",
        "\n",
        "for i, row in tqdm(enumerate(ragas_data), total=limit):\n",
        "    if i >= limit:\n",
        "        break\n",
        "    question = row[\"question\"]\n",
        "    ground_truths = row[\"ground_truth\"]\n",
        "    try:\n",
        "        out = chat(question)\n",
        "        answer = out[\"output\"]\n",
        "        if len(out[\"intermediate_steps\"]) != 0:\n",
        "            contexts = out[\"intermediate_steps\"][0][1].split(\"\\n---\\n\")\n",
        "        else:\n",
        "            # this is where no intermediate steps are used\n",
        "            contexts = []\n",
        "    except ValueError:\n",
        "        answer = \"ERROR\"\n",
        "        contexts = []\n",
        "    df = pd.concat([df, pd.DataFrame({\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"contexts\": [contexts],\n",
        "        \"ground_truth\": ground_truths\n",
        "    })], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "QrpnyBfuj0uL",
        "outputId": "79f7a5cf-4231-4729-ac9a-2ec965e4ade6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>answer</th>\n",
              "      <th>ground_truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input promp...</td>\n",
              "      <td>[The generative inference workload consists of...</td>\n",
              "      <td>The impact of encoding the input prompt on inf...</td>\n",
              "      <td>The encoding of the input prompt has an impact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inferenc...</td>\n",
              "      <td>[inference latency since the generation of tok...</td>\n",
              "      <td>Generating tokens sequentially during inferenc...</td>\n",
              "      <td>Generating tokens affects the inference speed ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B diff...</td>\n",
              "      <td>[Abstract\\nWe introduce Mixtral 8x7B, a Sparse...</td>\n",
              "      <td>Mixtral 8x7B differs from Mistral 7B primarily...</td>\n",
              "      <td>The architecture of Mixtral 8x7B differs from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for...</td>\n",
              "      <td>[1When deployed in 16-bit precision, Falcon-18...</td>\n",
              "      <td>Offloading in A100 servers for accelerating Mo...</td>\n",
              "      <td>Offloading is used on the A100 server for acce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in cod...</td>\n",
              "      <td>[Detailed results for Mistral 7B, Llama 2 7B/1...</td>\n",
              "      <td>Mixtral 8x7B significantly outperforms Llama 2...</td>\n",
              "      <td>Mixtral outperforms Llama 2 70B in code benchm...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the impact of encoding the input promp...   \n",
              "1  How does generating tokens affect the inferenc...   \n",
              "2  How does the architecture of Mixtral 8x7B diff...   \n",
              "3  When is offloading used on the A100 server for...   \n",
              "4  How does Mixtral compare to Llama 2 70B in cod...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [The generative inference workload consists of...   \n",
              "1  [inference latency since the generation of tok...   \n",
              "2  [Abstract\\nWe introduce Mixtral 8x7B, a Sparse...   \n",
              "3  [1When deployed in 16-bit precision, Falcon-18...   \n",
              "4  [Detailed results for Mistral 7B, Llama 2 7B/1...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  The impact of encoding the input prompt on inf...   \n",
              "1  Generating tokens sequentially during inferenc...   \n",
              "2  Mixtral 8x7B differs from Mistral 7B primarily...   \n",
              "3  Offloading in A100 servers for accelerating Mo...   \n",
              "4  Mixtral 8x7B significantly outperforms Llama 2...   \n",
              "\n",
              "                                        ground_truth  \n",
              "0  The encoding of the input prompt has an impact...  \n",
              "1  Generating tokens affects the inference speed ...  \n",
              "2  The architecture of Mixtral 8x7B differs from ...  \n",
              "3  Offloading is used on the A100 server for acce...  \n",
              "4  Mixtral outperforms Llama 2 70B in code benchm...  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGBS3rApYNIt",
        "outputId": "9bb59635-6f91-4ccf-ed5d-46229b94bb26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'contexts', 'answer', 'ground_truth'],\n",
              "    num_rows: 5\n",
              "})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_similarity,\n",
        "    answer_correctness,\n",
        ")\n",
        "\n",
        "eval_data = Dataset.from_dict(df)\n",
        "eval_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d645bb52c56b42ba88b78e3bc321aed6",
            "2d5f9179716a462ab4d1bd84808786e2",
            "17369a3ab6034a1683472f1c34d19b01",
            "bd003f9328f54f7fb52564a36364fe33",
            "46c5e86876fb435c913c95c4edbc2001",
            "434e98f493854e4c920d3bb316a82dc4",
            "63db45584d5d4f439832c40c7a7bb39f",
            "168a6e73679344da8a247cb6b65b09cf",
            "053d4d72f6c64153abbcbe908ca9fa27",
            "37d72aa109a442e2a59051e0328f48fa",
            "619d2d38fa6643cc9bce4aea7284d678"
          ]
        },
        "id": "gckieCTmzfg1",
        "outputId": "4b7d8912-0810-424e-8155-0b2d429c6cb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 30/30 [00:21<00:00,  1.37it/s]\n"
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=eval_data,\n",
        "    metrics=[\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        answer_similarity,\n",
        "        answer_correctness,\n",
        "    ],\n",
        ")\n",
        "result = result.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKuXPfTYwTr3"
      },
      "source": [
        "## Retrieval Metrics / Métricas de Recuperación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-pJwPjqwVrU"
      },
      "source": [
        "La recuperación (retrieval) es el primer paso en un pipeline RAG, por lo que nos enfocaremos primero en las métricas que evalúan la parte de recuperación. Para eso, queremos principalmente centrarnos en `context_recall` y `context_precision`, pero antes de entrar en esas métricas debemos entender qué miden exactamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Condición real (\"Actual\") vs. Predicha (\"Predicted\")\n",
        "\n",
        "Cuando evaluamos el rendimiento de sistemas de recuperación, tendemos a comparar los resultados _reales_ (ground truth) con los resultados _predichos_. Definimos estos términos de la siguiente manera:\n",
        "\n",
        "- **Condición real** es la etiqueta verdadera de cada contexto en el dataset. Estos son _positivos_ ($p$) si el contexto es relevante para la consulta, o _negativos_ ($n$) si el contexto es _irrelevante_.\n",
        "\n",
        "- **Condición predicha** es la etiqueta _predicha_ determinada por nuestro sistema de recuperación. Si un contexto es devuelto, es un _positivo predicho_ ($\\hat{p}$). Si no es devuelto, es un _negativo predicho_ ($\\hat{n}$).\n",
        "\n",
        "Dadas estas condiciones, podemos decir lo siguiente:\n",
        "\n",
        "- $p\\hat{p}$ es un **verdadero positivo**: un resultado relevante fue devuelto.\n",
        "- $n\\hat{n}$ es un **verdadero negativo**: un resultado irrelevante no fue devuelto.\n",
        "- $n\\hat{p}$ es un **falso positivo**: un resultado irrelevante fue devuelto.\n",
        "- $p\\hat{n}$ es un **falso negativo**: un resultado relevante _no_ fue devuelto.\n",
        "\n",
        "Veamos ahora cómo se aplican estos conceptos a nuestras métricas en RAGAS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MMDiXNOwi2n"
      },
      "source": [
        "#### Context Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHPVNBkwlMA"
      },
      "source": [
        "El context recall (o simplemente _recall_) es una medida de cuántos de los documentos relevantes en un conjunto han sido recuperados. Se calcula como:\n",
        "\n",
        "$$\n",
        "Recall@K = \\frac{p\\hat{p}}{p\\hat{p} + n\\hat{n}} = \\frac{Contextos \\: relevantes \\: recuperados}{Cantidad \\: total \\: de \\: contextos \\: relevantes}\n",
        "$$\n",
        "\n",
        "RAGAS calcula el _Recall@K_ para evaluar el recall, donde _@K_ representa la cantidad de contextos devueltos por el sistema. A medida que se incrementa el valor de @K, el recall mejora (ya que aumenta la cobertura del sistema de recuperación).\n",
        "\n",
        "En el extremo, podríamos establecer @K igual al tamaño total del dataset para garantizar un recall perfecto —aunque eso anula el propósito del enfoque RAG.\n",
        "\n",
        "Por defecto, RAGAS utiliza un valor de _@K_ igual a `5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZvbZv1IL3uR4",
        "outputId": "b98e2e94-5f82-4304-aa15-5c6bff921a70"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>answer</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>[The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\\nBelow, we look for ...</td>\n",
              "      <td>The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>[inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021)., â¢ The average number of ...</td>\n",
              "      <td>Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>[Abstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchm...</td>\n",
              "      <td>Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>[1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âkno...</td>\n",
              "      <td>Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>[Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : &lt; &lt;20 40 10 ay MMLU Knowledge Reasoning Compre...</td>\n",
              "      <td>Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                       question  \\\n",
              "0                                                   What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                How does generating tokens affect the inference speed in generative inference?   \n",
              "2  How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                        When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                   How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      contexts  \\\n",
              "0  [The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\\nBelow, we look for ...   \n",
              "1  [inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021)., â¢ The average number of ...   \n",
              "2  [Abstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchm...   \n",
              "3  [1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âkno...   \n",
              "4  [Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Compre...   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                     The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.   \n",
              "1                                                                                                                Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.   \n",
              "2  Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.   \n",
              "3                                                                                                                                                       Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.   \n",
              "4                                                                                                                                                                                                                                                                                                                        Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.   \n",
              "\n",
              "   context_recall  \n",
              "0             1.0  \n",
              "1             1.0  \n",
              "2             1.0  \n",
              "3             1.0  \n",
              "4             1.0  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option(\"display.max_colwidth\", 700)\n",
        "\n",
        "# Unimos el dataframe original (df) con las métricas (result)\n",
        "final_result = pd.concat([df.reset_index(drop=True), result.reset_index(drop=True)], axis=1)\n",
        "\n",
        "final_result[[\"question\", \"contexts\", \"answer\", \"context_recall\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA3mHXDO4ALO"
      },
      "source": [
        "Aquí podemos ver que todos los resultados devolvieron un `1.0`, lo cual indica que el 100% de los contextos relevantes fueron recuperados en cada caso.\n",
        "\n",
        "Como ejemplo hipotético, si se hubiera obtenido un score de `0.6`, eso significaría que se recuperaron 3 de cada 5 contextos relevantes (es decir, un 60%).\n",
        "\n",
        "El recall es una métrica útil, pero se puede engañar fácilmente si simplemente se devuelven más documentos, es decir, si se incrementa el valor de _@K_. Por este motivo, normalmente se combina con la métrica de precisión (precision)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmItVH9l_BeD"
      },
      "source": [
        "### Context Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnr0tS-7_Dho"
      },
      "source": [
        "La **precisión de contexto** (o simplemente _precisión_) es otra métrica muy popular para evaluar sistemas de recuperación. Es común ver **recall** y **precision** utilizadas en conjunto para evaluar el rendimiento de estos sistemas.\n",
        "\n",
        "Al igual que con el recall, la métrica real utilizada es _Precision@K_, donde **@K** representa la cantidad de contextos devueltos. Sin embargo, a diferencia del recall, la precisión se enfoca en la **cantidad de resultados relevantes dentro del total de resultados devueltos**, sin importar si los irrelevantes fueron descartados o no. Es decir, mide **la proporción de resultados relevantes entre todos los que se devolvieron**.\n",
        "\n",
        "$$\n",
        "Precision@K = \\frac{p\\hat{p}}{p\\hat{p} + p\\hat{n}} = \\frac{Contextos \\: relevantes \\: recuperados}{Total \\: de \\: contextos \\: devueltos}\n",
        "$$\n"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M6ZZgWPKWpw6",
        "outputId": "5e1163f5-d10a-46d6-a63d-3730175b6f52"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>answer</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>[The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\\nBelow, we look for ...</td>\n",
              "      <td>The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.</td>\n",
              "      <td>0.804167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>[inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021)., â¢ The average number of ...</td>\n",
              "      <td>Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>[Abstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchm...</td>\n",
              "      <td>Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>[1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âkno...</td>\n",
              "      <td>Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>[Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : &lt; &lt;20 40 10 ay MMLU Knowledge Reasoning Compre...</td>\n",
              "      <td>Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                       question  \\\n",
              "0                                                   What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                How does generating tokens affect the inference speed in generative inference?   \n",
              "2  How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                        When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                   How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      contexts  \\\n",
              "0  [The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\\nBelow, we look for ...   \n",
              "1  [inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021)., â¢ The average number of ...   \n",
              "2  [Abstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchm...   \n",
              "3  [1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âkno...   \n",
              "4  [Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Compre...   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                     The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.   \n",
              "1                                                                                                                Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.   \n",
              "2  Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.   \n",
              "3                                                                                                                                                       Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.   \n",
              "4                                                                                                                                                                                                                                                                                                                        Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.   \n",
              "\n",
              "   context_precision  \n",
              "0           0.804167  \n",
              "1           1.000000  \n",
              "2           1.000000  \n",
              "3           0.833333  \n",
              "4           0.887500  "
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option(\"display.max_colwidth\", 700)\n",
        "final_result[[\"question\", \"contexts\", \"answer\", \"context_precision\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD6NoHDZWyVx"
      },
      "source": [
        "RAGAS usa embeddings + soft matching, a diferencia de un sistema clásico donde un contexto es relevante o no (1 o 0).\n",
        "\n",
        "1. Calcula la similitud semántica entre los contextos devueltos y los contextos ideales (ground truths).\n",
        "\n",
        "2. Usa una métrica como cosine similarity para dar un score continuo (entre 0 y 1) a cada contexto recuperado.\n",
        "\n",
        "3. Suma esas similitudes para calcular la precisión, en vez de contar solo exact matches.\n",
        "\n",
        "Entonces, RAGAS no cuenta cuántos son “correctos”, sino que calcula un promedio ponderado,. Si hay un valor menor a 1, significa que algunos de los contextos eran parcialmente relevantes pero no perfectos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI5NgDpWXKuY"
      },
      "source": [
        "## Generation Metrics / Métricas de Generación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTUWxra6wK_A"
      },
      "source": [
        "### Faithfullness / Fidelidad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBqu0ZBGgAwl"
      },
      "source": [
        "La métrica de _faithfulness_ mide (en un rango de _0_ a _1_) la consistencia factual de una respuesta en relación con el contexto recuperado.  \n",
        "Un valor de _1_ significa que **todas las afirmaciones** presentes en la respuesta también se encuentran en el contexto.  \n",
        "Un valor de _0_ indica que **ninguna de las afirmaciones** de la respuesta se encuentra en el contexto.\n",
        "\n",
        "Calculamos _faithfulness_ de la siguiente manera:\n",
        "\n",
        "$$\n",
        "Faithfulness = \\frac{Número \\: de \\: afirmaciones \\: en \\: la \\: respuesta \\: que \\: también \\: están \\: en \\: el \\: contexto}{Número \\: total \\: de \\: afirmaciones \\: en \\: la \\: respuesta}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0rGhG62Hv2bm",
        "outputId": "ae9bf65e-97ca-4fde-a29e-132c709cb413"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>answer</th>\n",
              "      <th>faithfulness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>[The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many ...</td>\n",
              "      <td>The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.</td>\n",
              "      <td>0.777778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>[inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021)., â¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from a token generation perspective. Instead of focusing on the number of tokens in the prompt, it takes into account the number of tok...</td>\n",
              "      <td>Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>[Abstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on hum...</td>\n",
              "      <td>Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>[1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of ...</td>\n",
              "      <td>Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>[Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : &lt; &lt;20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code, Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es &amp; E60! Mistral 78 % 2...</td>\n",
              "      <td>Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                       question  \\\n",
              "0                                                   What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                How does generating tokens affect the inference speed in generative inference?   \n",
              "2  How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                        When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                   How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  contexts  \\\n",
              "0  [The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many ...   \n",
              "1  [inference latency since the generation of tokens cannot be parallelized. There is a bunch of literature addressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the model (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al., 2023b) and hardware (Wang et al., 2021)., â¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from a token generation perspective. Instead of focusing on the number of tokens in the prompt, it takes into account the number of tok...   \n",
              "2  [Abstract\\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on hum...   \n",
              "3  [1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of ...   \n",
              "4  [Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code, Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2...   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                     The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.   \n",
              "1                                                                                                                Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.   \n",
              "2  Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.   \n",
              "3                                                                                                                                                       Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.   \n",
              "4                                                                                                                                                                                                                                                                                                                        Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.   \n",
              "\n",
              "   faithfulness  \n",
              "0      0.777778  \n",
              "1      1.000000  \n",
              "2      0.888889  \n",
              "3      0.750000  \n",
              "4      1.000000  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option(\"display.max_colwidth\", 1000)\n",
        "final_result[[\"question\", \"contexts\", \"answer\", \"faithfulness\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQrPEENFg4j7"
      },
      "source": [
        "Cuando calculamos _faithfulness_, RAGAS utiliza modelos LLM de OpenAI para identificar qué afirmaciones están presentes en la respuesta y si también aparecen en el contexto.  \n",
        "Debido a la naturaleza \"generativa\" de este enfoque, no siempre obtendremos puntuaciones completamente precisas.\n",
        "\n",
        "Podemos observar que obtenemos puntuaciones perfectas en los casos 2 y 3, pero menores a 1 en los casos 1, 4 y 5. Sin embargo, al analizarlos, se pueden identificar afirmaciones que parecen estar relacionadas. Aun así, estas últimas respuestas parecen estar menos fundamentadas en el contexto que las otras, lo cual justifica esta menor puntuación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxZEFhW0h1FE"
      },
      "source": [
        "### Answer Relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIA7gdXqh2wF"
      },
      "source": [
        "La métrica **Answer Relevancy** (Relevancia de la respuesta) se enfoca en el componente de generación y es similar a la métrica de _context precision_, pero aplicada a la respuesta. Evalúa **qué tan relevante es la respuesta respecto a la pregunta original**.\n",
        "\n",
        "Se obtiene una **puntuación baja** de _answer relevancy_ cuando:\n",
        "\n",
        "- Las respuestas están **incompletas**.\n",
        "- Las respuestas contienen **información redundante o irrelevante**.\n",
        "\n",
        "Una **alta puntuación** en _answer relevancy_ indica que la respuesta es **concisa**, directa y **no contiene \"relleno\"** (es decir, información que no responde a la pregunta).\n",
        "\n",
        "#### ¿Cómo se calcula?\n",
        "\n",
        "Se le pide a un **modelo de lenguaje (LLM)** que genere varias preguntas hipotéticas a partir de la respuesta generada. Luego se calcula la **similitud del coseno** entre la **pregunta original** y estas **preguntas generadas**.  \n",
        "\n",
        "Naturalmente, si una respuesta es **precisa y específica**, las preguntas generadas tenderán a tener **alta similitud** con la original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "G_w-A1WOzddF",
        "outputId": "32c765a8-73c3-4e31-b1c9-7a8ead2c2cdf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_relevancy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.</td>\n",
              "      <td>0.989880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.</td>\n",
              "      <td>0.956186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.</td>\n",
              "      <td>0.907801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.</td>\n",
              "      <td>0.967998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.</td>\n",
              "      <td>0.937216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                       question  \\\n",
              "0                                                   What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                How does generating tokens affect the inference speed in generative inference?   \n",
              "2  How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                        When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                   How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              answer  \\\n",
              "0                                                                                                                                                     The impact of encoding the input prompt on inference speed in generative models is significant. Encoding the prompt involves processing the entire input in parallel, which is relatively efficient, especially with techniques like Mixture-of-Experts. However, during token generation, the model must process each token sequentially, loading parameters repeatedly, which can slow down inference. The speed is thus limited by how quickly system memory can fetch parameters, and optimizing expert loading patterns can help improve inference speed.   \n",
              "1                                                                                                                Generating tokens sequentially during inference significantly impacts speed because each token must be produced one after the other, rather than in parallel. This token-by-token process introduces latency, as each new token depends on the previous ones, making the overall inference slower. Additionally, the process involves repeatedly loading model parameters for each token, which can further slow down inference depending on system memory access speeds. Optimizations like model compression, efficient hardware, and improved loading strategies can help mitigate these delays.   \n",
              "2  Mixtral 8x7B differs from Mistral 7B primarily in its use of a sparse mixture of experts (SMoE) architecture, where each layer contains 8 feedforward (expert) blocks. During inference, only 13 billion active parameters are used per token, despite the model having access to a total of 47 billion parameters. In contrast, Mistral 7B is a dense model with a standard transformer architecture. This means Mixtral 8x7B can achieve similar or better performance with fewer active parameters during inference, leading to increased efficiency. The architecture modifications in Mixtral, such as the expert routing mechanism, distinguish it from the standard feedforward blocks used in Mistral 7B.   \n",
              "3                                                                                                                                                       Offloading in A100 servers for accelerating MoE-based language models is used during inference, especially when dealing with large models that require more memory than available on the GPU. Techniques such as expert caching, offloading experts to CPU memory, and overlapping expert loading with computation are employed to enable efficient inference on hardware with limited GPU memory. This approach allows models like Mixtral-8x7B to run interactively on consumer-grade hardware by reducing GPU memory usage and improving inference speed.   \n",
              "4                                                                                                                                                                                                                                                                                                                        Mixtral 8x7B significantly outperforms Llama 2 70B across most benchmarks, especially in code, mathematics, and reasoning tasks, while using approximately 5 times fewer active parameters during inference. This makes Mixtral more efficient in terms of compute cost, despite having a smaller active parameter count, and it achieves comparable or superior performance on many tasks.   \n",
              "\n",
              "   answer_relevancy  \n",
              "0          0.989880  \n",
              "1          0.956186  \n",
              "2          0.907801  \n",
              "3          0.967998  \n",
              "4          0.937216  "
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option(\"display.max_colwidth\", 700)\n",
        "final_result[[\"question\", \"answer\", \"answer_relevancy\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCXWYnBlr69a"
      },
      "source": [
        "Se observa una buena performance (similitud mayor a 0.9) para todas las preguntas evaluadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVaNDvKwl7BE"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
